
# The file renaming index is produced in a file, but then
# we need to look it up as a the workflow DAG is computed.
# Rather than having to read the entire file each time, we
# cache it in this module-level variable.
#
# I don't know whether it's thread safe to access a global
# structure like this in snakemake rules.  Since I'm in doubt
# we'll protect write accesses with a lock
from threading import Lock
_gRenameIndexLock = Lock()
_gRenameIndexCache = None

# The names of the final c4gh output files and the mapping input.c4gh ->
# output.c4gh is generated by this gen_rename_index rule.  Since parts of the
# DAG depend on these mappings, we mark this rule as a "checkpoint", so that
# its output can be used to generate/re-evaluate the DAG.
checkpoint gen_rename_index:
    """
    The workflow renames the files to be sent to uuid4 values.
    This rule generates the index for the dataset to be sent, mapping
    new random name to the original name.
    Index format:  one file per line; each line has the following tab-separated fields
        new name <tab> original name\n
    """
    output:
        index = "base_index.tsv"
    params:
        source_items = lambda _: [ str(path) for path in source_files ],
        source_ext = source_ext
    script:
        "../scripts/gen_rename_index.py"


def get_all_new_item_names():
    """
    Returns a list with all the output (uuid) file names
    from the index generated by checkpoint rule gen_rename_index.
    """
    with checkpoints.gen_rename_index.get().output.index.open() as f:
        return [ line.split("\t", 1)[0] for line in f ]


def get_original_item_name(new_name: str) -> str:
    global _gRenameIndexCache
    with _gRenameIndexLock:
        if _gRenameIndexCache is None:
            with checkpoints.gen_rename_index.get().output.index.open() as f:
                # each line in the file is a tab-separated tuple (new name, original name)
                _gRenameIndexCache = dict(line.rstrip('\n').split('\t', 1) for line in f)
    return _gRenameIndexCache[new_name]


rule final_index:
    """
    Creates the final data index by adding a new column to the base_index.
    The new column contains the checksums of the new, reencrypted files.

    Line order remains as in base_index, so the file is sorted by the first
    column (new randomly generated file name).
    """
    input:
        base_index = rules.gen_rename_index.output.index,
        data_files = lambda _: expand("reencrypted/{filename}.sha", filename=get_all_new_item_names())
    output:
        index = "index.tsv"
    run:
        with open(input.base_index) as base_index, open(output.index, 'w') as final_index:
            for line in base_index:
                rnd_name, original_name = line.rstrip('\n').split('\t')
                chksum_path = f"reencrypted/{rnd_name}.sha"
                with open(chksum_path) as chk_file:
                    chksum_value = chk_file.read().split(' ', 1)[0]
                new_index_line = f"{rnd_name}\t{original_name}\t{chksum_value}\n"
                final_index.write(new_index_line)
